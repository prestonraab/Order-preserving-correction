{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afddf6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from collections import Counter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85808924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagonal_merge(df1, df2):\n",
    "    \"\"\"   \n",
    "    Diagonally merging z-matrices    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if df1 is empty\n",
    "    if df1.empty:\n",
    "        return df2\n",
    "    \n",
    "    # Get the size of two dfs\n",
    "    rows1, cols1 = df1.shape\n",
    "    rows2, cols2 = df2.shape\n",
    "\n",
    "    # Create a combination of new and column names\n",
    "    new_index = list(df1.index) + list(df2.index)\n",
    "    new_columns = list(df1.columns) + list(df2.columns)\n",
    "\n",
    "    # Create a new DataFrame with rows and columns equal to the sum of the rows and columns of the two DataFrames, and fill in NaN\n",
    "    result = pd.DataFrame(0, index=new_index, columns=new_columns)\n",
    "\n",
    "    # Fill df1 into the upper left corner\n",
    "    result.iloc[:rows1, :cols1] = df1.values\n",
    "\n",
    "    # Fill df2 to the bottom right corner\n",
    "    result.iloc[rows1:, cols1:] = df2.values\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6776ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "\n",
    "def FindingNN(adata, k = 25):\n",
    "    \"\"\"   \n",
    "    Find the nearest neighbors within the same batch and the nearest neighbors between different batches    \n",
    "    args:\n",
    "        adata: Data\n",
    "        k: Number of neighbors, the default parameter is 25\n",
    "    \"\"\"    \n",
    "    \n",
    "    data = adata.X  \n",
    "    B = adata.obs[\"batch\"]  \n",
    "\n",
    "    # PCA dimensionality reduction\n",
    "    pca = PCA(n_components=100)\n",
    "    data_pca = pca.fit_transform(data)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    cosine_sim = cosine_similarity(data_pca)\n",
    "\n",
    "    # Create a DataFrame to save cosine similarity\n",
    "    cosine_df = pd.DataFrame(cosine_sim, index=adata.obs.index, columns=adata.obs.index)\n",
    "\n",
    "    # Batch Information\n",
    "    batches = B.unique()\n",
    "\n",
    "    # Store the nearest neighbors within the same batch and across batches\n",
    "    nearest_neighbors_same_batch = {}\n",
    "    nearest_neighbors_diff_batch = defaultdict(list)\n",
    "\n",
    "    # Calculate the nearest neighbors within the same batch\n",
    "    for batch in batches:\n",
    "        same_batch_cells = B[B == batch].index.tolist()\n",
    "        for cell in same_batch_cells:\n",
    "            same_batch_distances = cosine_df.loc[cell, same_batch_cells].drop(cell).nlargest(k)\n",
    "            nearest_neighbors_same_batch[cell] = same_batch_distances.index.tolist()\n",
    "\n",
    "    # Calculate the nearest neighbor across batches\n",
    "    for batch, other_batch in product(batches, batches):\n",
    "        if batch == other_batch:\n",
    "            continue\n",
    "        same_batch_cells = B[B == batch].index\n",
    "        other_batch_cells = B[B == other_batch].index\n",
    "        for cell in same_batch_cells:\n",
    "            neighbors = cosine_df.loc[cell, other_batch_cells].nlargest(k).index.tolist()\n",
    "            for neighbor in neighbors:\n",
    "                neighbor_neighbors = cosine_df.loc[neighbor, same_batch_cells].nlargest(k).index.tolist()\n",
    "                if cell in neighbor_neighbors:\n",
    "                    nearest_neighbors_diff_batch[cell].append(neighbor)\n",
    "\n",
    "    # Create adjacency matrix M\n",
    "    n = len(adata)\n",
    "    M = np.zeros((n, n), dtype=int)\n",
    "\n",
    "    for i, cell_i in enumerate(adata.obs_names):\n",
    "        same_batch_indices = [adata.obs_names.get_loc(neighbor) for neighbor in nearest_neighbors_same_batch.get(cell_i, [])]\n",
    "        M[i, same_batch_indices] = 1\n",
    "        M[same_batch_indices, i] = 1  \n",
    "\n",
    "        diff_batch_indices = [adata.obs_names.get_loc(neighbor) for neighbor in nearest_neighbors_diff_batch.get(cell_i, [])]\n",
    "        M[i, diff_batch_indices] = 1\n",
    "        M[diff_batch_indices, i] = 1 \n",
    "\n",
    "    return M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "551a7a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "def compute_S_matrix(Z_matrix_total, M):\n",
    "    \"\"\"\n",
    "    Construct similarity matrices between different clusters based on the z-matrix and adjacency matrix\n",
    "    args:    \n",
    "        Z_matrix_total: z-matrix\n",
    "        M: adjacency matrix\n",
    "    \"\"\"    \n",
    "\n",
    "    M_sparse = coo_matrix(M)\n",
    "    K = Z_matrix_total.shape[1]\n",
    "\n",
    "    # Initialize similarity matrix S\n",
    "    S = np.zeros((K, K))\n",
    "\n",
    "    for i, j in zip(M_sparse.row, M_sparse.col):\n",
    "        Z_i = Z_matrix_total.iloc[i].values \n",
    "        Z_j = Z_matrix_total.iloc[j].values  \n",
    "\n",
    "        S += np.outer(Z_i, Z_j)\n",
    "        \n",
    "    type_sums = Z_matrix_total.sum(axis=0).values  # 每列求和\n",
    "    for i in range(K):\n",
    "        for j in range(K):\n",
    "            if type_sums[i] != 0 and type_sums[j] != 0:\n",
    "                S[i, j] /= np.sqrt(type_sums[i] * type_sums[j])\n",
    "\n",
    "    np.fill_diagonal(S, 0)\n",
    "    S_df = pd.DataFrame(S, index=Z_matrix_total.columns, columns=Z_matrix_total.columns)\n",
    "\n",
    "    return S_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c1da4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "def nn_search(X, Y=None, k=25):\n",
    "    \"\"\"\n",
    "    Computes nearest neighbors in Y for points in X\n",
    "    args:\n",
    "        X: nxd tensor of query points\n",
    "        Y: mxd tensor of data points (optional)\n",
    "        k: number of neighbors\n",
    "    \"\"\"\n",
    "    if Y is None:\n",
    "        Y = X\n",
    "    X = X\n",
    "    Y = Y\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, algorithm='ball_tree').fit(Y)\n",
    "    Dis, Ids = nbrs.kneighbors(X)\n",
    "    return Dis, Ids\n",
    "def adjust_weights0(model,K = 3,number_of_items= 2000):\n",
    "    \"\"\"\n",
    "    Adjust weights to ensure monotonicity\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # 例如，确保layer1和layer2的第一个权重同号\n",
    "        for i in range(number_of_items):\n",
    "            if model.layer1.weight[K*i, i].item() * model.layer2.weight[i, K*i].item() < 0:\n",
    "                model.layer2.weight[i, K*i].neg_()\n",
    "            if model.layer1.weight[K*i+1, i].item() * model.layer2.weight[i, K*i+1].item() < 0:\n",
    "                model.layer2.weight[i, K*i+1].neg_()\n",
    "            if model.layer1.weight[K*i+2, i].item() * model.layer2.weight[i, K*i+2].item() < 0:\n",
    "                model.layer2.weight[i, K*i+2].neg_()\n",
    "            if model.layer3.weight[K*i, i].item() * model.layer4.weight[i, K*i].item() < 0:\n",
    "                model.layer4.weight[i, K*i].neg_()\n",
    "            if model.layer3.weight[K*i+1, i].item() * model.layer4.weight[i, K*i+1].item() < 0:\n",
    "                model.layer4.weight[i, K*i+1].neg_()\n",
    "            if model.layer3.weight[K*i+2, i].item() * model.layer4.weight[i, K*i+2].item() < 0:\n",
    "                model.layer4.weight[i, K*i+2].neg_()\n",
    "            if model.layer5.weight[K*i, i].item() * model.layer6.weight[i, K*i].item() < 0:\n",
    "                model.layer6.weight[i, K*i].neg_()\n",
    "            if model.layer5.weight[K*i+1, i].item() * model.layer6.weight[i, K*i+1].item() < 0:\n",
    "                model.layer6.weight[i, K*i+1].neg_()\n",
    "            if model.layer5.weight[K*i+2, i].item() * model.layer6.weight[i, K*i+2].item() < 0:\n",
    "                model.layer6.weight[i, K*i+2].neg_()\n",
    "def adjust_weights(model,K = 3,number_of_items= 2000):\n",
    "    \"\"\"\n",
    "    Adjust weights to ensure monotonicity\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # 例如，确保layer1和layer2的第一个权重同号\n",
    "        for i in range(number_of_items):\n",
    "            if model.layer1.weight[K*i, i].item() * model.layer2.weight[i, K*i].item() < 0:\n",
    "                model.layer2.weight[i, K*i]=0\n",
    "            if model.layer1.weight[K*i+1, i].item() * model.layer2.weight[i, K*i+1].item() < 0:\n",
    "                model.layer2.weight[i, K*i+1]=0\n",
    "            if model.layer1.weight[K*i+2, i].item() * model.layer2.weight[i, K*i+2].item() < 0:\n",
    "                model.layer2.weight[i, K*i+2]=0\n",
    "            if model.layer3.weight[K*i, i].item() * model.layer4.weight[i, K*i].item() < 0:\n",
    "                model.layer4.weight[i, K*i]=0\n",
    "            if model.layer3.weight[K*i+1, i].item() * model.layer4.weight[i, K*i+1].item() < 0:\n",
    "                model.layer4.weight[i, K*i+1]=0\n",
    "            if model.layer3.weight[K*i+2, i].item() * model.layer4.weight[i, K*i+2].item() < 0:\n",
    "                model.layer4.weight[i, K*i+2]=0\n",
    "            if model.layer5.weight[K*i, i].item() * model.layer6.weight[i, K*i].item() < 0:\n",
    "                model.layer6.weight[i, K*i]=0\n",
    "            if model.layer5.weight[K*i+1, i].item() * model.layer6.weight[i, K*i+1].item() < 0:\n",
    "                model.layer6.weight[i, K*i+1]=0\n",
    "            if model.layer5.weight[K*i+2, i].item() * model.layer6.weight[i, K*i+2].item() < 0:\n",
    "                model.layer6.weight[i, K*i+2]=0\n",
    "\n",
    "\n",
    "def gaussian_kernel(x, y, sigma):\n",
    "    \"\"\"\n",
    "    gaussian_kernel\n",
    "    args：\n",
    "        sigma: standard deviation\n",
    "    \"\"\"\n",
    "    beta = 1. / (0.5*sigma ** 2)\n",
    "    dist = torch.cdist(x, y)**2\n",
    "    return torch.exp(-beta * dist)\n",
    "    \n",
    "\n",
    "def compute_mmd(x, y, sigma1,sigma2,sigma3,Z1,Z2):\n",
    "    \"\"\"\n",
    "    Calculate the maximum mean divergence two sets of samples\n",
    "    args：\n",
    "        x: the first group of samples\n",
    "        y: the second group of samples\n",
    "        sigma1: the first standard deviation of a mixture Gaussian kernel\n",
    "        sigma2: the second standard deviation of a mixture Gaussian kernel\n",
    "        sigma3: the third standard deviation of a mixture Gaussian kernel\n",
    "        Z1: the weight of each sample in the first group of samples\n",
    "        Z2: the weight of each sample in the second group of samples\n",
    "    \"\"\"\n",
    "    x_kernel = gaussian_kernel(x, x, sigma1)+gaussian_kernel(x, x, sigma2)+gaussian_kernel(x, x, sigma3)\n",
    "    y_kernel = gaussian_kernel(y, y, sigma1)+gaussian_kernel(y, y, sigma2)+gaussian_kernel(y, y, sigma3)\n",
    "    xy_kernel = gaussian_kernel(x, y, sigma1)+gaussian_kernel(x, y, sigma2)+gaussian_kernel(x, y, sigma3)\n",
    "    Mask1 = torch.mm(Z1.unsqueeze(1), Z1.unsqueeze(1).t())\n",
    "    Mask2 = torch.mm(Z1.unsqueeze(1), Z2.unsqueeze(1).t())\n",
    "    Mask3 = torch.mm(Z2.unsqueeze(1), Z2.unsqueeze(1).t())\n",
    "    mmd = torch.sum(Mask1*x_kernel)/torch.sum(Mask1) + torch.sum(Mask3*y_kernel)/torch.sum(Mask3) - 2 * torch.sum(Mask2*xy_kernel)/torch.sum(Mask2)\n",
    "    return mmd\n",
    "    \n",
    "# Class: Calculate the maximum mean \n",
    "class MMDLoss(nn.Module):\n",
    "    def __init__(self, sigma1,sigma2,sigma3):\n",
    "        super(MMDLoss, self).__init__()\n",
    "        self.sigma1 = sigma1\n",
    "        self.sigma2 = sigma2\n",
    "        self.sigma3 = sigma3\n",
    "\n",
    "    def forward(self, x, y,Z1,Z2):\n",
    "        return compute_mmd(x, y,self.sigma1,self.sigma2,self.sigma3,Z1,Z2)\n",
    "\n",
    "# Initialize weights\n",
    "def init_weights_positive(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.uniform_(m.weight, a=-1/2, b=1/2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c12ee09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_columns(df, start_row, s, visited=None):\n",
    "    \"\"\"\n",
    "    Search for clusters that may be similar based on the similarity matrix and similarity threshold\n",
    "    args：\n",
    "        df: similarity matrix\n",
    "        start_row: a starting cluster\n",
    "        s: similarity threshold\n",
    "    \"\"\"\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "\n",
    "    results = []\n",
    "    columns1 = df.columns[df.loc[start_row] > s]\n",
    "    columns1 = [result for result in set(columns1) if result.startswith(start_row[0:6])]\n",
    "    columns2 = df.columns[df.loc[start_row] > s]\n",
    "    columns2 = [result for result in set(columns2) if not result.startswith(start_row[0:6])]\n",
    "\n",
    "    columns = columns1+columns2\n",
    "    for column in columns:\n",
    "        if column not in visited:\n",
    "            visited.add(column)\n",
    "            results.append(column)\n",
    "            additional_results, visited = find_columns(df, column, s,visited)\n",
    "            results.extend(additional_results)\n",
    "    return results, visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b23be192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_threshold(df, batch_name):\n",
    "    \"\"\"\n",
    "    Select similarity threshold for merging and matching clusters\n",
    "    args：\n",
    "        df: similarity matrix\n",
    "        batch_name: the name of query batch\n",
    "    \"\"\"\n",
    "    \n",
    "    for s in np.arange(0.5, 20, 0.05):\n",
    "        visited_all = []\n",
    "        visited_all_cur = 0\n",
    "        for start_cluster in df.columns[df.columns.str.contains(batch_name)]:\n",
    "            if start_cluster not in visited_all:\n",
    "                start_row = start_cluster\n",
    "                results, visited = find_columns(df, start_row, s)\n",
    "\n",
    "                # 将结果根据前缀分为两部分\n",
    "                batch1_results = [result for result in set(results) if result.startswith(batch_name[0:6])]\n",
    "                batch2_results = [result for result in set(results) if result.startswith('batch0_')]\n",
    "                # 提取数字并转换为整数\n",
    "                batch1_indices = [int(x.split('_')[1]) for x in batch1_results]\n",
    "                batch2_indices = [int(x.split('_')[1]) for x in batch2_results]\n",
    "                visited_all =  list(set(visited_all+batch1_results))\n",
    "                if len(batch2_indices)!=0:\n",
    "                    visited_all_cur = visited_all_cur+len(batch1_indices)+len(batch2_indices)\n",
    "        if s == 0.5:\n",
    "            visited_all_pre = visited_all_cur\n",
    "        if visited_all_cur < visited_all_pre:\n",
    "            break\n",
    "        else: threshold = s\n",
    "    return threshold\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "7d9aa078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def order_preserving_correction(adata, option = 'Global', preprocessing = False, methods = 'Louvain', ASW = True, resolution=1, epochs = 250):   \n",
    "    \"\"\"\n",
    "    Batch effect correction with the order-preserving feature\n",
    "    \n",
    "    args:\n",
    "        adata: The input data file in AnnData format, which can contain either single-cell RNA sequencing or bulk RNA sequencing data.\n",
    "        option: A string parameter that specifies the type of monotonic model to use. Set to 'Global' by default, which utilizes a global monotonic model. Alternatively, a 'Partial' model can be chosen.\n",
    "        preprocessing: A boolean parameter (default is False) that indicates whether to preprocess the data. It is recommended to preprocess the raw count data.\n",
    "        methods: A string that specifies the clustering algorithm for initialization. The default method is 'Louvain', with alternative options including 'Leiden' and 'GMM'.\n",
    "        ASW: A boolean parameter (default is True). When using the Louvain or Leiden algorithms, it determines the resolution parameter based on the Average Silhouette Width (ASW).\n",
    "        resolution: A numeric parameter (default is 1) that sets the custom resolution parameter when ASW is set to False.\n",
    "        epochs: An integer that defines the number of training iterations, with a default value of 250.    \n",
    "    \"\"\"   \n",
    "\n",
    "    random.seed(123)\n",
    "    batch_counts = adata.obs['batch'].value_counts()\n",
    "    # Create a new batch label\n",
    "    new_labels = {old: f'batch{i}' for i, old in enumerate(batch_counts.sort_values(ascending=False).index)}\n",
    "    # Replace the original batch label\n",
    "    adata.obs['batch'] = adata.obs['batch'].map(new_labels)\n",
    "    batches = Counter(adata.obs[\"batch\"])\n",
    "    most_common_batch = batches.most_common(1)[0][0]\n",
    "    batch_numbers = len(batches) \n",
    "    Z_matrix_total = pd.DataFrame()   \n",
    "    \n",
    "    if preprocessing == True:\n",
    "        sc.pp.filter_cells(adata, min_genes=10)\n",
    "        sc.pp.filter_genes(adata, min_cells=3)\n",
    "        sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4)\n",
    "        #sc.pp.log1p(adata)\n",
    "        adata.obs[\"batch\"] = adata.obs[\"batch\"].astype('category')\n",
    "        sc.pp.highly_variable_genes(adata, n_top_genes=2000, batch_key='batch')\n",
    "        adata = adata[:,adata.var[\"highly_variable\"]==True]\n",
    "    \n",
    "\n",
    "\n",
    "    if methods == 'Louvain':\n",
    "        for batch in adata.obs['batch'].unique():\n",
    "            adata_batch = adata[adata.obs['batch'] == batch]\n",
    "            sc.pp.pca(adata_batch)\n",
    "            sc.pp.neighbors(adata_batch)\n",
    "            sc.tl.umap(adata_batch)\n",
    "            if ASW == True:\n",
    "                asw_score_pre = 0\n",
    "                asw_score_pre2 = 0\n",
    "                for resolution in np.arange(0.05, 3, 0.01):\n",
    "                    sc.tl.louvain(adata_batch, resolution=resolution)\n",
    "                    X = adata_batch.obsm['X_pca']\n",
    "                    Y = adata_batch.obsm['X_umap']\n",
    "                    labels = adata_batch.obs['louvain']\n",
    "                    if len(labels.value_counts()) == 1:\n",
    "                        break\n",
    "                    asw_score = silhouette_score(X, labels)\n",
    "                    asw_score2 = silhouette_score(Y, labels)\n",
    "                    if (asw_score < asw_score_pre)&(asw_score2 < asw_score_pre2):\n",
    "                        resolution = resolution - 0.01\n",
    "                        print(resolution)\n",
    "                        break\n",
    "                    else:\n",
    "                        asw_score_pre = asw_score\n",
    "                        asw_score_pre2 = asw_score2\n",
    "            sc.tl.louvain(adata_batch, resolution=resolution)\n",
    "            adata_batch.zmatrix = pd.get_dummies(adata_batch.obs['louvain'])\n",
    "            num_cols = adata_batch.zmatrix.shape[1]\n",
    "            adata_batch.zmatrix.columns = ['%s_%d' % (batch, i) for i in range(num_cols)]\n",
    "            adata_batch.zmatrix.index = np.where(adata.obs['batch'] ==  batch)[0]\n",
    "            Z_matrix_total = diagonal_merge(Z_matrix_total, adata_batch.zmatrix)\n",
    "\n",
    "        \n",
    "    \n",
    "    if methods == 'Leiden':\n",
    "        for batch in adata.obs['batch'].unique():\n",
    "            adata_batch = adata[adata.obs['batch'] == batch]\n",
    "            sc.pp.pca(adata_batch)\n",
    "            sc.pp.neighbors(adata_batch)\n",
    "            sc.tl.umap(adata_batch)\n",
    "            if ASW == True:\n",
    "                asw_score_pre = 0\n",
    "                asw_score_pre2 = 0\n",
    "                for resolution in np.arange(0.05, 3, 0.01):\n",
    "                    sc.tl.leiden(adata_batch, resolution=resolution)\n",
    "                    X = adata_batch.obsm['X_pca']\n",
    "                    Y = adata_batch.obsm['X_umap']\n",
    "                    labels = adata_batch.obs['leiden']\n",
    "                    if len(labels.value_counts()) == 1:\n",
    "                        break\n",
    "                    asw_score = silhouette_score(X, labels)\n",
    "                    asw_score2 = silhouette_score(Y, labels)\n",
    "                    if (asw_score < asw_score_pre)&(asw_score2 < asw_score_pre2):\n",
    "                        resolution = resolution - 0.01\n",
    "                        break\n",
    "                    else:\n",
    "                        asw_score_pre = asw_score\n",
    "                        asw_score_pre2 = asw_score2\n",
    "            sc.tl.leiden(adata_batch, resolution=resolution)\n",
    "            adata_batch.zmatrix = pd.get_dummies(adata_batch.obs['leiden'])\n",
    "            num_cols = adata_batch.zmatrix.shape[1]\n",
    "            adata_batch.zmatrix.columns = ['%s_%d' % (batch, i) for i in range(num_cols)]\n",
    "            adata_batch.zmatrix.index = np.where(adata.obs['batch'] ==  batch)[0]\n",
    "            Z_matrix_total = diagonal_merge(Z_matrix_total, adata_batch.zmatrix)\n",
    "         \n",
    "    if methods == 'GMM':\n",
    "        import numpy as np\n",
    "        from sklearn.mixture import GaussianMixture\n",
    "        def gmm_with_bic(X, max_components=20):\n",
    "            best_gmm = None\n",
    "            best_bic = np.inf  \n",
    "            best_n_components = 1\n",
    "\n",
    "            for n_components in range(2, max_components + 1):\n",
    "                gmm = GaussianMixture(n_components=n_components, random_state=123)\n",
    "                gmm.fit(X)\n",
    "\n",
    "                bic = gmm.bic(X)  # 计算BIC\n",
    "\n",
    "                # If the current BIC is smaller, update the best model\n",
    "                if bic < best_bic:\n",
    "                    best_bic = bic\n",
    "                    best_gmm = gmm\n",
    "                    best_n_components = n_components\n",
    "                    \n",
    "            # Return the posterior probability matrix Z of the best model\n",
    "            Z_matrix = best_gmm.predict_proba(X)\n",
    "            \n",
    "            return Z_matrix\n",
    "            \n",
    "        for batch in adata.obs['batch'].unique():\n",
    "            adata_batch = adata[adata.obs['batch'] == batch]\n",
    "            sc.pp.pca(adata)\n",
    "            sc.pp.neighbors(adata_batch) \n",
    "            sc.tl.umap(adata_batch)\n",
    "            adata_batch.zmatrix = pd.DataFrame(gmm_with_bic(adata_batch.obsm[\"X_umap\"]))\n",
    "            num_cols = adata_batch.zmatrix.shape[1]\n",
    "            adata_batch.zmatrix.columns = ['%s_%d' % (batch, i) for i in range(num_cols)]\n",
    "            adata_batch.zmatrix.index = np.where(adata.obs['batch'] ==  batch)[0]\n",
    "            Z_matrix_total = diagonal_merge(Z_matrix_total, adata_batch.zmatrix)\n",
    "\n",
    "            \n",
    "    M = FindingNN(adata)\n",
    "    S = compute_S_matrix(Z_matrix_total, M)\n",
    "    \n",
    "    # set it to True to use GPU and False to use CPU\n",
    "    use_gpu = True  \n",
    "    if use_gpu:\n",
    "        torch.cuda.set_device(0)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # the number of genes\n",
    "    number_of_items = adata.X.shape[1]\n",
    "    \n",
    "    # the number of nodes\n",
    "    K = 3  \n",
    "    \n",
    "    if option == 'Partial':\n",
    "        class Generator(nn.Module):\n",
    "            def __init__(self,G):\n",
    "                super(Generator, self).__init__()\n",
    "                self.layer1 = nn.Linear(in_features=number_of_items+G, out_features=K*number_of_items)\n",
    "                self.layer2 = nn.Linear(in_features=K*number_of_items, out_features=number_of_items)\n",
    "                nn.init.zeros_(self.layer1.bias)\n",
    "                nn.init.zeros_(self.layer2.bias)\n",
    "                self.leaky_relu = nn.LeakyReLU(negative_slope=0.1)\n",
    "                self.layer3 = nn.Linear(in_features=number_of_items+G, out_features=K*number_of_items)\n",
    "                self.layer4 = nn.Linear(in_features=K*number_of_items, out_features=number_of_items)\n",
    "                nn.init.zeros_(self.layer3.bias)\n",
    "                nn.init.zeros_(self.layer4.bias)\n",
    "                self.layer5 = nn.Linear(in_features=number_of_items+G, out_features=K*number_of_items)\n",
    "                self.layer6 = nn.Linear(in_features=K*number_of_items, out_features=number_of_items)\n",
    "                nn.init.zeros_(self.layer5.bias)\n",
    "                nn.init.zeros_(self.layer6.bias)\n",
    "                self.bias1 = nn.Parameter(torch.zeros(number_of_items))\n",
    "                self.bias2 = nn.Parameter(torch.zeros(number_of_items))\n",
    "                self.bias3 = nn.Parameter(torch.zeros(number_of_items))\n",
    "\n",
    "            def forward(self, x,label):\n",
    "                inputs =  torch.cat(dim = 1, tensors = [x,label])\n",
    "                h1 = F.linear(inputs, self.layer1.weight*mask1, self.layer1.bias)\n",
    "                outputs1 = F.linear(h1, self.layer2.weight*mask2, self.layer2.bias)\n",
    "                outputs1 = self.leaky_relu(outputs1)\n",
    "                outputs2 = F.linear(h1, self.layer2.weight*mask3/number_of_items, self.bias1)\n",
    "                output = outputs1+MASK2*outputs2/number_of_items\n",
    "\n",
    "                output = torch.cat(dim = 1, tensors = [output,label])\n",
    "                output = F.linear(output, self.layer3.weight*mask1, self.layer3.bias)\n",
    "                outputs1 = F.linear(output, self.layer4.weight*mask2, self.layer4.bias)\n",
    "                outputs1 = self.leaky_relu(outputs1)\n",
    "                outputs2 = F.linear(output, self.layer4.weight*mask3/number_of_items, self.bias2)        \n",
    "                output = outputs1+MASK2*outputs2/number_of_items\n",
    "                output = x/2+output\n",
    "\n",
    "                output = torch.cat(dim = 1, tensors = [output,label])\n",
    "                h1 = F.linear(output, self.layer5.weight*mask1, self.layer5.bias)\n",
    "                outputs1 = F.linear(h1, self.layer6.weight*mask2, self.layer6.bias)\n",
    "                outputs1 = self.leaky_relu(outputs1)\n",
    "                outputs2 = F.linear(h1, self.layer6.weight*mask3/number_of_items, self.bias3)\n",
    "                return outputs1+MASK2*outputs2/number_of_items\n",
    "\n",
    "    if option == 'Global':\n",
    "        class Generator(nn.Module):\n",
    "            def __init__(self):\n",
    "                super(Generator, self).__init__()\n",
    "                self.layer1 = nn.Linear(in_features=number_of_items, out_features=K*number_of_items)\n",
    "                self.layer2 = nn.Linear(in_features=K*number_of_items, out_features=number_of_items)\n",
    "                nn.init.zeros_(self.layer1.bias)\n",
    "                nn.init.zeros_(self.layer2.bias)\n",
    "                self.leaky_relu = nn.LeakyReLU(negative_slope=0.1)\n",
    "                self.layer3 = nn.Linear(in_features=number_of_items, out_features=K*number_of_items)\n",
    "                self.layer4 = nn.Linear(in_features=K*number_of_items, out_features=number_of_items)\n",
    "                nn.init.zeros_(self.layer3.bias)\n",
    "                nn.init.zeros_(self.layer4.bias)\n",
    "                self.layer5 = nn.Linear(in_features=number_of_items, out_features=K*number_of_items)\n",
    "                self.layer6 = nn.Linear(in_features=K*number_of_items, out_features=number_of_items)\n",
    "                nn.init.zeros_(self.layer5.bias)\n",
    "                nn.init.zeros_(self.layer6.bias)\n",
    "                self.bias1 = nn.Parameter(torch.zeros(number_of_items))\n",
    "                self.bias2 = nn.Parameter(torch.zeros(number_of_items))\n",
    "                self.bias3 = nn.Parameter(torch.zeros(number_of_items))\n",
    "\n",
    "            def forward(self, x):\n",
    "                inputs =  x\n",
    "                h1 = F.linear(inputs, self.layer1.weight*mask1, self.layer1.bias)\n",
    "                outputs1 = F.linear(h1, self.layer2.weight*mask2, self.layer2.bias)\n",
    "                outputs1 = self.leaky_relu(outputs1)\n",
    "                outputs2 = F.linear(h1, self.layer2.weight*mask3,self.bias1)\n",
    "                output = outputs1+MASK2*outputs2/number_of_items\n",
    "\n",
    "                output = output\n",
    "                output = F.linear(output, self.layer3.weight*mask1, self.layer3.bias)\n",
    "                outputs1 = F.linear(output, self.layer4.weight*mask2, self.layer4.bias)\n",
    "                outputs1 = self.leaky_relu(outputs1)\n",
    "                outputs2 = F.linear(output, self.layer4.weight*mask3,self.bias2)\n",
    "                output = outputs1+MASK2*outputs2/number_of_items\n",
    "\n",
    "                output = x/2+output\n",
    "                output = F.linear(output, self.layer5.weight*mask1, self.layer5.bias)\n",
    "                outputs1 = F.linear(output, self.layer6.weight*mask2, self.layer6.bias)\n",
    "                outputs1 = self.leaky_relu(outputs1)\n",
    "                outputs2 = F.linear(output, self.layer6.weight*mask3, self.bias3)\n",
    "\n",
    "                return outputs1+MASK2*outputs2/number_of_items\n",
    "\n",
    "    if option == 'Partial':\n",
    "        import numpy as np \n",
    "        reference_batch = pd.DataFrame(adata.X[adata.obs['batch'] == 'batch0'])\n",
    "        batch_name =  adata.obs['batch'].unique()\n",
    "        query_batch_name = batch_name[batch_name != 'batch0']\n",
    "        resultall = reference_batch\n",
    "        train_2 = reference_batch\n",
    "        target_data = torch.tensor(train_2.values).to(device)\n",
    "        Dis, Ids = nn_search(train_2,k=25)\n",
    "        sigma1 = np.percentile(Dis.mean(axis=1), 50)/2\n",
    "        sigma2 = np.percentile(Dis.mean(axis=1), 50)\n",
    "        sigma3 = np.percentile(Dis.mean(axis=1), 50)*2\n",
    "        mmdloss = MMDLoss(sigma1=sigma1,sigma2=sigma2,sigma3=sigma3)\n",
    "        for z in query_batch_name:\n",
    "            train_1 = pd.DataFrame(adata.X[adata.obs['batch'] == z])\n",
    "            source_data = torch.tensor(train_1.values).to(device)\n",
    "            MASK1 = torch.ones((train_1.shape[0],train_1.shape[1])).to(device)\n",
    "            MASK2 = torch.ones((train_1.shape[0],train_1.shape[1])).to(device)\n",
    "            MASK1[source_data==0]=0\n",
    "            MASK2[source_data!=0]=0\n",
    "            g = Z_matrix_total.columns.str.contains(z).sum()\n",
    "            mask1 =  torch.ones((K*number_of_items,number_of_items+g)).to(device)\n",
    "            for i in range(number_of_items):\n",
    "                mask1[0:(K*i),i]=0\n",
    "                mask1[(K*i+K):(K*number_of_items),i]=0\n",
    "            mask2 =  torch.ones((number_of_items,K*number_of_items)).to(device)\n",
    "            for i in range(number_of_items):\n",
    "                mask2[i,0:(K*i)]=0\n",
    "                mask2[i,(K*i+K):(K*number_of_items)]=0    \n",
    "\n",
    "            mask3 =  torch.zeros((number_of_items,K*number_of_items)).to(device)\n",
    "            for i in range(number_of_items):\n",
    "                mask3[i,0:(K*i)]=1\n",
    "                mask3[i,(K*i+K):(K*number_of_items)]=1        \n",
    "\n",
    "            seed = 123  \n",
    "            np.random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "            \n",
    "            generator = Generator(G=g).to(device)\n",
    "            generator.apply(init_weights_positive)\n",
    "            adjust_weights0(generator,K = 3,number_of_items=number_of_items)\n",
    "            \n",
    "            lr = 0.01\n",
    "            g_optimizer = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "            \n",
    "            g_loss_values = []\n",
    "            threshold = find_threshold(S,z)\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "                if Z_matrix_total.empty:\n",
    "                    raise ValueError(\"Z_matrix_total is not defined or is empty.\")\n",
    "                if use_gpu is True:\n",
    "                    source_data = torch.tensor(train_1.values).to(device)\n",
    "                    target_data = torch.tensor(train_2.values).to(device)\n",
    "                    Z1_all = torch.tensor(Z_matrix_total.loc[(adata.obs == z).values,Z_matrix_total.columns.str.contains(z)].values).to(device)\n",
    "                    Z2_all = torch.tensor(Z_matrix_total.loc[(adata.obs == 'batch0').values,Z_matrix_total.columns.str.contains('batch0')].values).to(device)\n",
    "                else:\n",
    "                    source_data = torch.tensor(train_1.values)\n",
    "                    target_data = torch.tensor(train_2.values)\n",
    "                    Z1_all = torch.tensor(Z_matrix_total.loc[(adata.obs == z).values,Z_matrix_total.columns.str.contains(z)].values)\n",
    "                    Z2_all = torch.tensor(Z_matrix_total.loc[(adata.obs == 'batch0').values,Z_matrix_total.columns.str.contains('batch0')].values)\n",
    "\n",
    "\n",
    "                Z1_all =Z1_all.float()\n",
    "                Z2_all =Z2_all.float()\n",
    "                source_data = source_data.float()\n",
    "                target_data = target_data.float()\n",
    "                Label_source = Z1_all.float()\n",
    "\n",
    "\n",
    "                # Training Generator\n",
    "                g_optimizer.zero_grad()\n",
    "                g_output = generator(source_data,Label_source)\n",
    "\n",
    "\n",
    "                g_loss = 0\n",
    "                visited_all = []\n",
    "                for start_cluster in Z_matrix_total.columns[Z_matrix_total.columns.str.contains(z)]:\n",
    "                    if start_cluster not in visited_all:\n",
    "                        start_row = start_cluster\n",
    "                        results, _ = find_columns(S, start_row,s = threshold)\n",
    "\n",
    "                        # Divide the results into two parts based on batch information\n",
    "                        batch1_results = [result for result in set(results) if result.startswith(z)]\n",
    "                        batch2_results = [result for result in set(results) if result.startswith('batch0_')]\n",
    "                        batch1_indices = [int(x.split('_')[1]) for x in batch1_results]\n",
    "                        batch2_indices = [int(x.split('_')[1]) for x in batch2_results]\n",
    "                        visited_all = visited_all+batch1_results\n",
    "                        g_loss = g_loss + mmdloss(g_output,target_data,Z1_all[:,batch1_indices].sum(dim=1),Z2_all[:,batch2_indices].sum(dim=1))                \n",
    "                g_loss.backward()\n",
    "                g_optimizer.step()\n",
    "\n",
    "                if (epoch+1) % 5 == 0:\n",
    "                    adjust_weights(generator,K = 3,number_of_items=number_of_items)\n",
    "                    print(f'Epoch {epoch+1}/{epochs},  Generator Loss2: {g_loss.item()}')\n",
    "\n",
    "\n",
    "            g_output_numpy = generator(source_data,Label_source).cpu().detach().numpy()\n",
    "            result = pd.DataFrame(g_output_numpy)\n",
    "            result.columns = resultall.columns\n",
    "            resultall = pd.concat([resultall,result], axis=0)\n",
    "    if option == 'Global':  \n",
    "        import numpy as np \n",
    "        reference_batch = pd.DataFrame(adata.X[adata.obs['batch'] == 'batch0'])\n",
    "        batch_name =  adata.obs['batch'].unique()\n",
    "        query_batch_name = batch_name[batch_name != 'batch0']\n",
    "        resultall = reference_batch\n",
    "        train_2 = reference_batch\n",
    "        target_data = torch.tensor(train_2.values).to(device)\n",
    "        Dis, Ids = nn_search(train_2,k=25)\n",
    "        sigma1 = np.percentile(Dis.mean(axis=1), 50)/2\n",
    "        sigma2 = np.percentile(Dis.mean(axis=1), 50)\n",
    "        sigma3 = np.percentile(Dis.mean(axis=1), 50)*2\n",
    "        mmdloss = MMDLoss(sigma1=sigma1,sigma2=sigma2,sigma3=sigma3)\n",
    "        for z in query_batch_name:\n",
    "            train_1 = pd.DataFrame(adata.X[adata.obs['batch'] == z])\n",
    "            source_data = torch.tensor(train_1.values).to(device)\n",
    "            MASK1 = torch.ones((train_1.shape[0],train_1.shape[1])).to(device)\n",
    "            MASK2 = torch.ones((train_1.shape[0],train_1.shape[1])).to(device)\n",
    "            MASK1[source_data==0]=0\n",
    "            MASK2[source_data!=0]=0\n",
    "            mask1 =  torch.ones((K*number_of_items,number_of_items)).to(device)\n",
    "            for i in range(number_of_items):\n",
    "                mask1[0:(K*i),i]=0\n",
    "                mask1[(K*i+K):(K*number_of_items),i]=0\n",
    "            mask2 =  torch.ones((number_of_items,K*number_of_items)).to(device)\n",
    "            for i in range(number_of_items):\n",
    "                mask2[i,0:(K*i)]=0\n",
    "                mask2[i,(K*i+K):(K*number_of_items)]=0    \n",
    "\n",
    "            mask3 =  torch.zeros((number_of_items,K*number_of_items)).to(device)\n",
    "            for i in range(number_of_items):\n",
    "                mask3[i,0:(K*i)]=1\n",
    "                mask3[i,(K*i+K):(K*number_of_items)]=1        \n",
    "\n",
    "            seed = 123\n",
    "            np.random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "            \n",
    "            generator = Generator().to(device)\n",
    "            generator.apply(init_weights_positive)\n",
    "            adjust_weights0(generator,K = 3,number_of_items=number_of_items)\n",
    "            \n",
    "            lr = 0.01\n",
    "            g_optimizer = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "            \n",
    "            g_loss_values = []\n",
    "            threshold = find_threshold(S,z)\n",
    "\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "                if Z_matrix_total.empty:\n",
    "                    raise ValueError(\"Z_matrix_total is not defined or is empty.\")\n",
    "                if use_gpu is True:\n",
    "                    source_data = torch.tensor(train_1.values).to(device)\n",
    "                    target_data = torch.tensor(train_2.values).to(device)\n",
    "                    Z1_all = torch.tensor(Z_matrix_total.loc[(adata.obs == z).values,Z_matrix_total.columns.str.contains(z)].values).to(device)\n",
    "                    Z2_all = torch.tensor(Z_matrix_total.loc[(adata.obs == 'batch0').values,Z_matrix_total.columns.str.contains('batch0')].values).to(device)\n",
    "                else:\n",
    "                    source_data = torch.tensor(train_1.values)\n",
    "                    target_data = torch.tensor(train_2.values)\n",
    "                    Z1_all = torch.tensor(Z_matrix_total.loc[(adata.obs == z).values,Z_matrix_total.columns.str.contains(z)].values)\n",
    "                    Z2_all = torch.tensor(Z_matrix_total.loc[(adata.obs == 'batch0').values,Z_matrix_total.columns.str.contains('batch0')].values)\n",
    "\n",
    "\n",
    "                Z1_all =Z1_all.float()\n",
    "                Z2_all =Z2_all.float()\n",
    "                source_data = source_data.float()\n",
    "                target_data = target_data.float()\n",
    "\n",
    "\n",
    "                # 训练生成器\n",
    "                g_optimizer.zero_grad()\n",
    "                g_output = generator(source_data)\n",
    "\n",
    "\n",
    "                g_loss = 0\n",
    "                visited_all = []\n",
    "                for start_cluster in Z_matrix_total.columns[Z_matrix_total.columns.str.contains(z)]:\n",
    "                    if start_cluster not in visited_all:\n",
    "                        start_row = start_cluster\n",
    "                        results, _ = find_columns(S, start_row,s = threshold)\n",
    "\n",
    "                        # Divide the results into two parts based on batch information\n",
    "                        batch1_results = [result for result in set(results) if result.startswith(z)]\n",
    "                        batch2_results = [result for result in set(results) if result.startswith('batch0_')]\n",
    "                        batch1_indices = [int(x.split('_')[1]) for x in batch1_results]\n",
    "                        batch2_indices = [int(x.split('_')[1]) for x in batch2_results]\n",
    "                        visited_all = visited_all+batch1_results\n",
    "                        g_loss = g_loss + mmdloss(g_output,target_data,Z1_all[:,batch1_indices].sum(dim=1),Z2_all[:,batch2_indices].sum(dim=1))                \n",
    "                g_loss.backward()\n",
    "                g_optimizer.step()\n",
    "\n",
    "                if (epoch+1) % 5 == 0:\n",
    "                    adjust_weights(generator,K = 3,number_of_items=number_of_items)\n",
    "                    print(f'Epoch {epoch+1}/{epochs},  Generator Loss2: {g_loss.item()}')\n",
    "\n",
    "\n",
    "            g_output_numpy = generator(source_data).cpu().detach().numpy()\n",
    "            result = pd.DataFrame(g_output_numpy)\n",
    "            result.columns = resultall.columns\n",
    "            resultall = pd.concat([resultall,result], axis=0)\n",
    "    return resultall\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
